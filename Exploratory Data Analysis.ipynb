{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the dataset\n",
    "\n",
    "\n",
    "In this notebook, we will perform an EDA (Exploratory Data Analysis) on the processed Waymo dataset (data in the `processed` folder). In the first part, you will create a function to display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_dataset\n",
    "import IPython.display as display\n",
    "import matplotlib\n",
    "# Necessary to see matplotlib outside of container\n",
    "import tkinter\n",
    "matplotlib.use('TKAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = get_dataset(\"/mnt/data/processed/*.tfrecord\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function to display an image and the bounding boxes\n",
    "\n",
    "Implement the `display_instances` function below. This function takes a batch as an input and display an image with its corresponding bounding boxes. The only requirement is that the classes should be color coded (eg, vehicles in red, pedestrians in blue, cyclist in green)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import IPython.display as display\n",
    "import glob\n",
    "from helpers.visualization import visualize_tf_record_dataset\n",
    "from helpers.exploratory_analysis import display_structure_of_dataset_item, show_dataset_basics\n",
    "\n",
    "\n",
    "def display_instances(batch):\n",
    "    \"\"\"\n",
    "    This function takes a batch from the dataset and display the image with \n",
    "    the associated bounding boxes.\n",
    "    \"\"\"\n",
    "    # ADD CODE HERE\n",
    "    for idx, sample in enumerate(batch):\n",
    "        print(f\"Printing image {idx}\")        \n",
    "        plt.imshow(sample['image'].numpy())\n",
    "\n",
    "\n",
    "def parse_record(record):\n",
    "    '''Function to parse one record.'''\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/source_id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/bbox/xmin': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/class/text': tf.io.VarLenFeature(dtype=tf.string),\n",
    "        'image/object/class/label': tf.io.VarLenFeature(dtype=tf.int64),\n",
    "    }\n",
    "    return tf.io.parse_single_example(record, image_feature_description)\n",
    "\n",
    "\n",
    "def transform_record(record):\n",
    "    ret_dict = {\n",
    "        'image': tf.image.decode_image(record['image/encoded']).numpy(),\n",
    "        'filename': record['image/filename'].numpy(),\n",
    "        'width': record['image/width'].numpy(),\n",
    "        'height': record['image/height'].numpy(),\n",
    "        'classes_text': record['image/object/class/text'].values.numpy(),\n",
    "        'classes': record['image/object/class/label'].values.numpy()\n",
    "    }\n",
    "    # boxes in data as [0,1], has to be multiplied with width / height\n",
    "    boxes_odd = np.array([\n",
    "        record['image/object/bbox/ymin'].values.numpy()*ret_dict['height'],\n",
    "        record['image/object/bbox/xmin'].values.numpy()*ret_dict['width'],\n",
    "        record['image/object/bbox/ymax'].values.numpy()*ret_dict['height'],\n",
    "        record['image/object/bbox/xmax'].values.numpy()*ret_dict['width'],\n",
    "    ])\n",
    "    ret_dict['boxes'] = [boxes_odd[:, idx]\n",
    "                         for idx in range(boxes_odd.shape[1])]\n",
    "    return ret_dict\n",
    "\n",
    "\n",
    "def project1_visualize_inspect(tf_record_path_array):\n",
    "    '''Function to visualize and inspect dataset according to project1'''\n",
    "    raw_image_dataset = tf.data.TFRecordDataset(tf_record_path_array)\n",
    "    display_structure_of_dataset_item(raw_image_dataset)\n",
    "\n",
    "    parsed_image_dataset = raw_image_dataset.map(parse_record)\n",
    "    # Transform to numpy/python if wanted\n",
    "    transformed_dataset = [transform_record(\n",
    "        element) for element in parsed_image_dataset]\n",
    "\n",
    "    show_dataset_basics(transformed_dataset)\n",
    "\n",
    "    # Debug Visu\n",
    "    if False:\n",
    "        for image_element in transformed_dataset[0:2]:\n",
    "            plt.imshow(image_element['image'])\n",
    "            plt.show()\n",
    "            # display.display(display.Image(data=image_element['image']))\n",
    "\n",
    "    # Visualize\n",
    "    visualize_tf_record_dataset(\n",
    "        transformed_dataset,\n",
    "        n_show=100,\n",
    "        x_max=3, y_max=4,\n",
    "        show_gt_class_names=True,\n",
    "        class_names=['', 'car', 'pedestrian', '', 'bike'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display 10 images \n",
    "\n",
    "Using the dataset created in the second cell and the function you just coded, display 10 random images with the associated bounding boxes. You can use the methods `take` and `shuffle` on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STUDENT SOLUTION HERE\n",
    "all_tf_records = glob.glob('/mnt/data/processed/*.tfrecord')\n",
    "project1_visualize_inspect(all_tf_records)  # all_tf_records[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional EDA\n",
    "\n",
    "In this last part, you are free to perform any additional analysis of the dataset. What else would like to know about the data?\n",
    "For example, think about data distribution. So far, you have only looked at a single file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
